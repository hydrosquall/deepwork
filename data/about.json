{"title":"About Us","intro":{"content":[{"text":"Words and code in this project are by me, Jess Peter. Editing is by Rob Smith and designs are by Matt Daniels. Our extraordinarily talented illustrator has chosen to remain anonymous."},{"text":"While deepwork isn’t a real company (yet), all of the technology used to generate our feature demos comes from real, easy to access code you can find online, and all of the grief comes from real trends observed in at least some corporate settings. That said, it’s not that serious. If you enjoyed the article, keep reading for more about my process."},{"text":"The pictures we used were as follows:"}],"list":[{"name":"\"Woman in pink crew-neck in closeup photography”","credit":"Mathias Huysmans","link":"https://unsplash.com/photos/U4JDjYmjn1g"},{"name":"\"Woman in white dress shirt\"","credit":"Christina @ wocintechchat.com","link":"https://unsplash.com/photos/sw3FSL9hIoI"},{"name":"\"Man in gray and black checkered sport shirt and black pants outfit\"","credit":"Himanshu Dewangan","link":"https://unsplash.com/photos/k9tUQNeOfx0"},{"name":"\"Man standing beside wall\"","credit":"LinkedIn Sales Solutions","link":"https://unsplash.com/photos/pAtA8xe_iVM"},{"name":"\"Man wearing black and teal dress suit standing near gray wall\"","credit":"Gregory Hayes","link":"https://unsplash.com/photos/h5cd51KXmRQ"},{"name":"\"Woman in black and white striped shirt sitting on chair\"","credit":"Ty Feague","link":"https://unsplash.com/photos/o-NNN0xQiSU"},{"name":"\"Man wearing gray suit jacket and dress pants\"","credit":"Roland Samuel (no longer online)"},{"name":"\"Woman smiling and sitting\"","credit":"Christina @ wocintechchat.com","link":"https://unsplash.com/photos/PlikkWB79qs"},{"name":"\"Happy senior man giving thumb up, sitting at desk using laptop computer at home.\"","credit":"StockLite","link":"https://www.shutterstock.com/image-photo/happy-senior-man-giving-thumb-sitting-73143208"}]},"subtitle":"Our Technology","resume":{"title":"Resume Atelier","content":[{"text":"Initially, I had wanted to finetune a GPT-2 model on resume data myself, similar to what I did in the Thought Leadership demo. Unfortunately, even though your LinkedIn profile information is definitely available online and it’s perfectly legal to continue to scrape that data for now, I really didn’t want to insert myself or The Pudding team into that type of debacle. The remaining freely available datasets hosted on sources like Kaggle I found either too sparse or too specific to work with (at least at the time of writing)."},{"text":"Fortunately, Max Woolf, Thomas Davis, and the folks behind JSON Resume already did the hard work of creating an open-source fake resume generator based on a recurrent neural network. Their models were trained off of approximately 6,000 real job resumes, alluded to be resumes hosted on Github gists (perhaps explaining the technical lean of the resumes generated)."},{"text":"All the resumes generated in this feature were trained off of the same models provided by the authors. The differences in the experience levels are represented only in the amount of data produced for certain fields (e.g., “Student” resumes generally have fewer examples of work experience than “Intermediate” or “Senior” resumes) and “Students” work at specific roles for shorter periods of time overall."}]},"photo":{"title":"Profile Picture Studio","content":[{"text":"The features used in Profile Picture studio are all based on the same principle. In lay terms, you take a model trained on many photos of people and computationally create a new artificial face that closely resembles your photo of choice. Then you tweak the newly generated face. In not so lay terms, this is called projecting the photo into the latent space of the model. Jason Brownlee provides a reasonable explanation of the technique on his website (for those of us with interest, but without the patience to parse an academic paper)."},{"text":"The demos in this article were made using Github user Woctezuma’s code for projecting and altering photos. Woctezuma uses NVIDIA’s StyleGAN2 model which has been trained on 70,000 photos of people sourced from Flickr (known as FFHQ)."},{"text":"Though I discussed it briefly in the article, I wanted to again acknowledge the fact that the “Drab” to “Fab” filter generally gives subjects lighter skin and predominantly European features the “fabber” it is set. I do not equate white skin with physical beauty, and neither does The Pudding. This is a consequence of the underlying technology used. In writing this explanation, I don’t mean to excuse the implications of this filters’ use nor others like it, but it is beyond my capabilities to fix it and I wanted to include some representation of digital “beautification.”"},{"text":"Despite my personal inability to “fix” the issue of racial and other biases in data science involving images of people (e.g., consider this write-up on artist Jake Elwes’s work around queer representation in datasets used in data science), I wanted to provide a bit more information on how this phenomenon comes about, particularly around beauty filters and beauty assessment."}]},"photo-more":{"content":[{"text":"The specific beautification attributes I used comes from Woctezuma’s code, who in turn used code from a user’s project called “seeprettyface.” The website for seeprettyface notes that for creating the labeled data required to determine a generated picture’s attributes (e.g., beauty, gender presentation, smile, etc.), they used a combination of Microsoft’s image recognition API and Baidu Cloud. Baidu Cloud’s Chinese language API documentation describes features for measuring and enhancing the image subjects’ beauty. I could not find any documentation in Microsoft’s API documentation on beauty measurement or manipulation, which suggests to me that the measurements of beauty seeprettyface used to create the labeled data necessary to make the “beautification” filters I used comes from Baidu."},{"text":"I cannot find a specific reference that identifies how Baidu measures beauty. This may be a failure of my searching, a language barrier, or because the company wishes to protect trade secrets. In lieu of specific information on Baidu’s methods, I can speak to some of the methods and factors that impact other popular beauty filters or digital measurements."},{"text":"It is important to understand that in order to measure or create a filter for facial beauty, Baidu’s algorithm very likely first needed to find or create some form of labeled data on facial beauty, as is common practice in most machine-learning tasks. They may have created their own dataset from scratch and had people rate each picture, used an existing dataset of faces and had people rate those pictures, or they may have used a dataset created specifically for facial beauty perception that already contains peoples’ ratings for the subjects’ attractiveness. It’s possible that Baidu used a different method, unlikely given the state of the art."},{"text":"Many sources, including experts speaking on the AI beauty contest in the Guardian article linked in the main text, describe lack of representation of minorities in underlying datasets as a key issue in algorithmically measuring or creating beauty. This is a real issue. Though the dataset FFHQ described previously uses images posted on Flickr, other datasets are more narrow in the subjects represented. For example, the CelebFaces Attributes (CelebA) Dataset created by researchers at The Chinese University of Hong Kong is popular for a variety of data science tasks related to human faces. One paper associated with the creation and use of the dataset was cited by over 4,500 people at the time of writing according to Google Scholar. The popular machine learning library TensorFlow includes CelebA in its data catalog."},{"text":"As implied by its name, CelebFaces Attributes contains pictures of over 200,000 celebrities’ faces. Though the creators don’t appear to disclose how they chose which celebrities to include, the images appear to be of predominantly Western celebrities. This is supported by sources like the creators of the Diversity in Faces (DiF) dataset, who determined that about 86% of subjects pictured in CelebA had lighter skin tones, and about 78% of subjects were under the age of 46–which doesn’t exactly sound out of line with the demographics of Western celebrities. The authors behind DiF found similar levels of unbalanced age and skin tone representation in other popular face datasets."},{"text":"We can also consider as an example ​​the SCUT-FBP5500 dataset–a dataset created specifically for algorithmically measuring facial beauty–contains only pictures of Asian and Caucasian subjects. Each photo in SCUT-FBP5500 was given a score between 1-5 for beauty from a team of volunteers aged between 18-27, which invites us to consider the bias introduced by those determining what constitutes beauty in datasets labeled for those purposes."},{"text":"A final dataset worth considering in this discussion is the HotOrNot dataset. In this dataset, the authors use a subset of photos and their ratings from the website HotOrNot.com. HotOrNot was a website that invited users to rate the hotness of user-submitted photos from 1-10. Mashable recounts its rise to fame and eventual downfall in this piece honoring the site’s 20th birthday. Anecdotally, as a former dumbass kid with the open mindedness of an early 2000s Friends episode, HotOrNot was mostly popular with dumbass kids with the open mindedness of early 2000s Friends episodes. Though not as popular as the other datasets described, the HotOrNot dataset has been used in research on measurement or adjustment of facial “beauty,” such as in this paper on a method for facial beauty prediction."},{"text":"In reality, the researchers behind all of these datasets bear little of the blame for the lack of representation in their data or methods. It’s a normal practice for researchers to scope their experiments just so that they can test a specific thing. One intended use of CelebA, for example, was to develop and test a method for identifying a person’s face in a variety of different circumstances. So, it makes sense that the authors opted to use subjects that it is easy to find pictures of. One output discussed in the creation of SCUT-FBP5500 is to provide data that can specifically be used to discuss the difference in perception of beauty in Asian subjects versus Caucasian subjects. Though not explicitly stated by its creators, it is likely the HotOrNot dataset was conceived as a way of gaining a large number of images that had already been rated by a large number of people (the authors only used images with more than 100 ratings) as these kind of tasks can be time consuming and expensive, particularly for that amount of input."},{"text":"Though there’s certainly more many researchers can do to address lack of representation and other bias in their datasets, much of the folly that comes from attempting to algorithmically measure or enhance beauty–beyond the question of why?–stems from the misuse of data produced for specific tests under specific circumstances. Or, in some cases, the issue may stem from the lazy creation of new datasets for general-purpose commercial products without due consideration of the biases introduced by minimal diversity in the subjects pictures, or in preferences introduced by a largely homogenous group evaluating the images. We can see the results of these issues in the biases found in everything from AI beauty contests, to commercial product APIs, to satirical essays on corporate culture."}]},"thought-leadership":{"title":"Thought Leadership Package","content":[{"text":"The Thought Leadership package uses OpenAI’s GPT-2 text generation model trained on a dataset composed of 8 million web pages that have been posted on Reddit. I used Max Woolf’s excellent gpt-2-simple Python library to simplify the process."},{"text":"To generate text specific to my selected thought influencers, I downloaded each thought leader’s 3,200 most recent Tweets (the limit imposed by the Twitter API). I excluded replies and retweets. Due to this, and potentially due to the user’s lower total Tweet count, some users had fewer Tweets to train on than others. I then fine tuned the GPT-2 model on Tweets from each specified combination of thought leader. Due to the uneven distribution of the number of Tweets available for each user, some of the generated outputs resemble one author more than the other. The selection of thought leaders was not based on any structured methodology, rather, they were nominated to me by friends or found through Google search."}]},"video-call":{"title":"Video Conferencing Plug-in","content":[{"text":"The Video Conferencing Plug-in uses the First Order Motion Model framework to animate still photos to act out recorded facial expressions. I wrote code based on the paper authors’ demo to develop the visuals for this feature."}]}}